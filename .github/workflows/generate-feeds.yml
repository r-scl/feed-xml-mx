name: Generate Product Feeds v2.0

on:
  schedule:
    # Ejecutar cada día a las 2 AM (hora de México)
    - cron: '0 8 * * *'  # 8 UTC = 2 AM México
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Permite ejecutar manualmente
    inputs:
      enable_scraping:
        description: 'Enable web scraping'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  generate-feeds:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Generate enhanced feeds (v2.0)
      env:
        ENABLE_SCRAPING: ${{ github.event.inputs.enable_scraping || 'false' }}
      run: python feed_processor_v2.py
    
    - name: Upload feeds as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: product-feeds-v2
        path: output/
        retention-days: 30
    
    - name: Setup Pages
      if: github.ref == 'refs/heads/main'
      uses: actions/configure-pages@v4
      
    - name: Upload Pages artifact
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./output
        
  deploy:
    if: github.ref == 'refs/heads/main'
    needs: generate-feeds
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4